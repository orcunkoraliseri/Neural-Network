'''Overfitting and underfitting tutorial
link: https://github.com/sibirbil/VeriDefteri/blob/master/Asiri_Eksik_Ogrenme/Asiri_Eksik_Ogrenme.ipynb
   Regression of Linear Model
'''
import warnings
warnings.simplefilter('ignore')

import html
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

# linear regression and gradient boosting
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingClassifier

# classification
from sklearn.datasets import make_classification

# error and cost function
from sklearn.metrics import mean_squared_error, accuracy_score

from sklearn.preprocessing import PolynomialFeatures

# DATA GENERATION (there is linear correlation between each other)
num_obs = 75 # number of observations
np.random.seed(1)
X = np.random.rand(num_obs)*10
y = 2.5 * X + np.random.rand(num_obs)*5

# divide test (%20) and train (%80) data
X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8,random_state=0)

'''
# visualise raw input data
plt.figure(figsize=(8,8))
plt.scatter(X_train,y_train)
plt.show()
'''

# MODEL
lr = LinearRegression()
lr.fit(X_train.reshape(-1,1), y_train.reshape(-1,1)) # fit the data
print("Intercept: " + str(lr.intercept_))
print("Slope: " + str(lr.coef_[0]))

# results of model between [0,10]
line_X = np.arange(0,11,0.1)
line_y = lr.predict(line_X.reshape(-1,1))

plt.figure(figsize=(8,8))
plt.scatter(X_train,y_train, alpha = 0.5, label = 'Train')
plt.scatter(X_test, y_test, c='red', alpha = 0.7, label = 'Test')
plt.plot(line_X, line_y, c='green', linewidth=3, alpha=0.6, label = 'Linear Regression')
plt.legend(loc = 4)
plt.show()

