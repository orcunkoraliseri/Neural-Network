link1=https://hub.coursera-notebooks.org/user/kxwsckmtvyzuocblmevjym/notebooks/Week%202/Python%20Basics%20with%20Numpy/Python%20Basics%20With%20Numpy%20v3.ipynb
link2=https://github.com/amarsic1990/Deep-learning/blob/master/Python%20Basics%20with%20numpy.py

import math as m
import numpy as np

def basic_sigmoid(x):
    s = 1 / (1 + m.exp(-x))
    print(s)
    return s
#x = np.array([1, 2, 3])
#basic_sigmoid(3)

# possible to print array list
def sigmoid(x):
    s = 1 / (1 + np.exp(-x))
    print(s)
    return s
x = np.array([1,2,3])
#sigmoid(x)

# sigmoid gradient
# optimize loss function using backpropogation
def sigmoid_derivative(x):
    s = 1 / (1 + np.exp(-x))
    # derivative sigmoid = sigmoid * (1 - sigmoid)
    ds = s * (1 - s)
    print(ds)
    return s
x = np.array([1,2,3])
#sigmoid_derivative(x)

# reshaping arrays
# unroll image to vector
# X.shape is used to get the shape (dimension) of a matrix/vector X
# X.reshape(...) is used to reshape X into some other dimension
def image2vector(image):
    v = image.reshape(image.shape[0] * image.shape[1] * image.shape[2],1)
    print("image2vector(image) = " + str(v))
    return v
image = np.array([[[ 0.67826139,  0.29380381],
        [ 0.90714982,  0.52835647],
        [ 0.4215251 ,  0.45017551]],

       [[ 0.92814219,  0.96677647],
        [ 0.85304703,  0.52351845],
        [ 0.19981397,  0.27417313]],

       [[ 0.60659855,  0.00533165],
        [ 0.10820313,  0.49978937],
        [ 0.34144279,  0.94630077]]])
#image2vector(image)

# normalize data
# It often leads to a better performance because gradient descent converges
# faster after normalization
def normalizeRows(x):
    x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)
    x = x / x_norm
    print(x)
    return x
x = np.array([[0, 3, 4], [1, 6, 4]])
#normalizeRows(x)

# Broadcasting: it describes how numpy treats arrays with different shapes
# during arithmetic operations
# https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html
# softmax
def softmax(x):
    x_exp = np.exp(x)
    x_sum = np.sum(x_exp, axis=1, keepdims=True)
    s = x_exp / x_sum
    print(s)
    return s
x = np.array([[9,2,5,0,0],[7,5,0,0,0]])
#softmax(x)


# VECTORIZATION
import  time as t

x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]
x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]
# classic dot product of vectors implementation
tic = t.process_time()
dot = 0
for i in range(len(x1)):
    dot += x1[i]*x2[i]
toc = t.process_time()
#print("dot= " + str(dot) + " " + "Computation time= " + str(1000*(toc - tic)) + "ms")

# versus vectorized dot product of vectors implementation
tic = t.process_time()
dot = np.dot(x1, x2)
toc = t.process_time()
#print("dot= " + str(dot) + " " + "Computation time= " + str(1000*(toc - tic)) + "ms")


# Loss function: L1
# It is used to evaluate the performance of the prediction model
# by minimizing absolute differences
def L1(yhat, y):
    loss1 = np.sum(abs(y - yhat))
    return loss1
yhat = np.array([.9, .2, .1,.4,.9])
y = np.array([1,0,0,1,1])
print("L1= " + str(L1(yhat, y)))


# Loss function: L2
# It is used to evaluate the performance of the prediction model
# by minimizing squared differences
def L2(yhat, y):
    loss2 = np.sum((y - yhat)**2)
    return loss2
yhat = np.array([.9, .2, .1,.4,.9])
y = np.array([1,0,0,1,1])
print("L2= " + str(L2(yhat, y)))
